{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import sys\n",
    "sys.path.append('./functions')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from functions.checks_and_preprocessing.missing_or_nan import check_missing_or_nan\n",
    "from functions.checks_and_preprocessing.stationarity_normality import kpss_adf_stationarity, normality_testing\n",
    "from functions.data_load_and_transform.sql_connections import get_database_connector, get_beach_data\n",
    "from functions.plotting.data_and_acf import create_widgets_and_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select one beach (via index), read it's data from the SQL database, and check for missing hours and NaN values\n",
    "DATA_STARTDATE = \"1979-01-01\"\n",
    "DATA_ENDDATE = \"2021-12-31\"\n",
    "\n",
    "single_beach_data, beach_name_sql_table = get_beach_data(get_database_connector())\n",
    "check_missing_or_nan(single_beach_data, beach_name_sql_table, DATA_STARTDATE, DATA_ENDDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = [0.05, .10, .25, .5, .75, .90, .95]\n",
    "single_beach_data.describe(percentiles=percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check if there are zero-valued or negative-valued features\n",
    "zero_var_list = []\n",
    "negative_var_list = []\n",
    "for variable in single_beach_data.columns:\n",
    "    if any(single_beach_data[variable] == 0):\n",
    "        zero_var_list.append(variable)\n",
    "    if any(single_beach_data[variable] < 0):\n",
    "        negative_var_list.append(variable)\n",
    "\n",
    "print(f'Features with zero values: {zero_var_list}') \n",
    "print(f'Features with negative values: {negative_var_list}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Kwiatkowski-Phillips-Schmidt-Shin (KPSS) and Augmented Dickey-Fuller (ADF) stationarity tests \n",
    "# on every column\n",
    "stationarity_dict = {}\n",
    "for col in single_beach_data.columns:\n",
    "    kpssh, adfh = kpss_adf_stationarity(single_beach_data[col])\n",
    "    stationarity_dict[col] = [kpssh, adfh]\n",
    "\n",
    "pprint.pprint(stationarity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their paper “Applying LSTM to Time Series Predictable through Time-Window Approaches”, Gers, Eck and Schmidhuber claim \n",
    "\"Our results suggest to use LSTM only on tasks where traditional time window-based approaches must fail.\n",
    "LSTM’s ability to track slow oscillations in the chaotic signal may be applicable to cognitive domains such as rhythm detection in speech and music.\"\n",
    "Based on that, we do not apply differentiation to non-stationary columns at this point, seeing as how LTSM's could excel at handling such datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carry out 4 normality tests\n",
    "normality_dict = {}\n",
    "for col in single_beach_data.columns:\n",
    "    normality_results = normality_testing(single_beach_data[col])\n",
    "    normality_dict[col] = normality_results\n",
    "\n",
    "pprint.pprint(normality_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking distribution of each column. Binning done with Freedman-Diaconis Rule.\n",
    "\n",
    "# Calculate the number of rows needed for subplots\n",
    "num_vars = len(single_beach_data.columns)\n",
    "num_rows = math.ceil(num_vars / 3)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=num_rows, ncols=3, figsize=(15, num_rows*5))\n",
    "\n",
    "# Flatten the axes array and iterate over it and the columns at the same time\n",
    "for ax, column in zip(axs.flatten(), single_beach_data.columns):\n",
    "    IQR = single_beach_data[column].quantile(0.75) - single_beach_data[column].quantile(0.25)\n",
    "    h = 2 * IQR * (len(single_beach_data[column])**(-1/3))\n",
    "    num_bins = int((single_beach_data[column].max() - single_beach_data[column].min()) / h)\n",
    "    \n",
    "    single_beach_data[column].plot(kind='hist', bins=num_bins, ax=ax)\n",
    "    ax.set_title(f'Distribution of {column}')\n",
    "\n",
    "# Remove unused subplots\n",
    "for i in range(num_vars, num_rows*3):\n",
    "    fig.delaxes(axs.flatten()[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = single_beach_data.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "plt.figure(figsize=(16, 11))\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plot to help visualize the data and ACF/PACF, with customizable options.\n",
    "create_widgets_and_plot(single_beach_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scuba_map",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
