{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from functions.checks_and_preprocessing.lagging_and_splitting import split_dataframe, sliding_window\n",
    "from functions.models.models_and_training import create_multiple_LSTM, train_model\n",
    "from functions.data_load_and_transform.sql_connections import get_database_connector, get_beach_data\n",
    "from functions.plotting.forecast_plot import plot_forecast\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization assumes that your observations fit a Gaussian distribution (bell curve) with a well behaved mean and standard deviation. \n",
    "You can still standardize your time series data if this expectation is not met, but you may not get reliable results.\n",
    "\n",
    "Make histogram to check if standartization or normalization should be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The experimental run of the LSTM should be run 10 times, as LSTM is stochastic. Or set a random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Beach Details:\n",
      "силистар\n"
     ]
    }
   ],
   "source": [
    "single_beach_data, beach_name_sql_table = get_beach_data(get_database_connector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function, where we specify the temporal-scale of the data, which return the normalized version of that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_beach_data_w = single_beach_data.resample(\"W\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test, test_index = split_dataframe(single_beach_data_w)\n",
    "features = len(single_beach_data_w.columns)\n",
    "\n",
    "#Scaling should be in a func\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "valid_scaled = scaler.transform(valid)\n",
    "test_scaled =scaler.transform(test)\n",
    "\n",
    "df_scaled = scaler.transform(single_beach_data_w.values)\n",
    "#Reverse scaling should also be a func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 21\n",
    "\n",
    "trainX, trainY = sliding_window(train_scaled, window_size)\n",
    "valX, valY = sliding_window(valid_scaled, window_size)\n",
    "testX, testY = sliding_window(test_scaled, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_models(layers: list, units: list, window: int = window_size, features: int = features) -> dict:\n",
    "    models = {}\n",
    "\n",
    "    for n_layers in layers:\n",
    "        for n_units in units:\n",
    "            model_name = f'{n_layers} layers, {n_units} units'\n",
    "            models[model_name] = {\n",
    "                'model': create_multiple_LSTM(n_layers=n_layers, units=n_units, window=window, features=features),\n",
    "                'history': None\n",
    "            }\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1, 2, 3, 4, 5]\n",
    "units = [100, 150, 200, 250, 300]\n",
    "\n",
    "models = generate_models(layers=layers, units=units, window=window_size, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_info in models.values():\n",
    "    model_info['history'] = train_model(model=model_info['model'], trainX=trainX, trainY=trainY, valX=valX, valY=valY, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(models: dict, metric: str = 'root_mean_squared_error') -> dict:\n",
    "    best_model_name = None\n",
    "    best_model_history = None\n",
    "    best_metric_value = float('inf')\n",
    "\n",
    "    for model_name, model_info in models.items():\n",
    "        if model_info['history'] is not None:\n",
    "            metric_values = model_info['history'].history[metric]\n",
    "            if min(metric_values) < best_metric_value:\n",
    "                best_metric_value = min(metric_values)\n",
    "                best_model_name = model_name\n",
    "                best_model_history = model_info['history']\n",
    "\n",
    "    return {best_model_name: best_model_history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_history = get_best_model(models, 'root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code get_best_models, so that all models are trained and stored in a dictionary, with all their best value, with the top values circled and best model selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix the plot_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast(models, list(best_model_history.keys())[0], testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT on if I can use multiindex df for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post this line it needs to be changed, due to the multi model change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "valPredict = model.predict(valX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "#Check which of these have been used so far. Not sure for testY ValY, etc.\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform(trainY)\n",
    "\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform(testY)\n",
    "\n",
    "valPredict = scaler.inverse_transform(valPredict)\n",
    "valY = scaler.inverse_transform(valY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "trainScore = math.sqrt(mean_squared_error(trainY.ravel(), trainPredict.ravel()))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "\n",
    "testScore = math.sqrt(mean_squared_error(testY.ravel(), testPredict.ravel()))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loss, validation loss/ training, validation accuracy plot as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check this\n",
    "#train_loss, train_acc = model.evaluate_generator(train_generator, steps=16) not _generator\n",
    "#validation_loss, test_acc = model.evaluate_generator(validation_generator, steps=16)\n",
    "#print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to plot the loss per column. Not bad, even needed.\n",
    "num_columns = trainY.shape[1]\n",
    "\n",
    "# Create subplots for each column\n",
    "fig, axes = plt.subplots(num_columns, 1, figsize=(10, 5*num_columns), sharex=True)\n",
    "\n",
    "# Loop through each column and plot the actual vs. predicted values\n",
    "for col in range(num_columns):\n",
    "    actual = trainY[:, col]\n",
    "    predicted = trainPredict[:, col]\n",
    "\n",
    "    # Plot actual values in blue\n",
    "    axes[col].plot(actual, label='Actual', color='blue')\n",
    "    \n",
    "    # Plot predicted values in orange\n",
    "    axes[col].plot(predicted, label='Predicted', color='orange')\n",
    "    \n",
    "    # Add labels and legends\n",
    "    axes[col].set_title(f'Column {col+1}')\n",
    "    axes[col].set_xlabel('Sample')\n",
    "    axes[col].set_ylabel('Value')\n",
    "    axes[col].legend()\n",
    "\n",
    "# Adjust layout for better readability\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tip: ACF - window size that aligns with these significant lags to capture these patterns of seasonality or temporality \n",
    "#Tip2: PACF - how many previous time steps to include in your window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering:\n",
    "\n",
    "#Lag Features: Create lag features (i.e., features with past values of the target variable) to capture autocorrelation. Experiment with different lag values to see which ones are most informative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scuba_map",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
