{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the names of all the beaches along the Bulgarian coast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The url of the page\n",
    "ROOT_URL = 'https://pochivka.bg/plazhove-bulgaria-f120'\n",
    "\n",
    "#Define some options for the driver for a better execution\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "\n",
    "driver = webdriver.Chrome(options = chrome_options) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Page is loading dynamically, so we have to simulate scrolling behaviour\n",
    "def anti_ad(url):    \n",
    "    driver = webdriver.Chrome(options = chrome_options) \n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "    \n",
    "    scroll_count = 0\n",
    "    while scroll_count < 6:   #We need all the rows to load, so we scroll 6 times to the end of the page\n",
    "        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "        time.sleep(1)   #Give the content some time to load\n",
    "        scroll_count += 1\n",
    "\n",
    "    #Remove pop-up and backdrop\n",
    "    POPUP_ELEMENT = driver.find_element(By.CSS_SELECTOR, \"div.fixed-box.quiz\") \n",
    "    driver.execute_script(\"arguments[0].remove();\", POPUP_ELEMENT)\n",
    "\n",
    "    \n",
    "    BACKDROP_ELEMENT = driver.find_element(By.CSS_SELECTOR, 'div.backdrop[style*=\"display: block;\"]')\n",
    "    driver.execute_script(\"arguments[0].remove();\", BACKDROP_ELEMENT)  \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DRIVER = anti_ad(ROOT_URL)\n",
    "driver.quit()\n",
    "\n",
    "#Get the HTML of the page\n",
    "ROOT_PAGE_HTML = ROOT_DRIVER.page_source\n",
    "soup = BeautifulSoup(ROOT_PAGE_HTML, \"lxml\")\n",
    "\n",
    "#Get the names for each beach\n",
    "PATTERN = r'span class=\"map\">\\s*<img alt=\"([^\"]*)\"'\n",
    "beach_list = re.findall(PATTERN, str(soup))\n",
    "\n",
    "\n",
    "#Remove the (плаж) from the list\n",
    "REMOVE_STRING = ' (плаж)'\n",
    "beach_list = [item.replace(REMOVE_STRING, '') for item in beach_list]\n",
    "\n",
    "\n",
    "#Make the list into a dictionary, keys are beach names\n",
    "beach_dict = {'beach_name': beach_list}\n",
    "\n",
    "#Get the URLs for each beach and append them to the dict \n",
    "TITLE_DIVS = ROOT_DRIVER.find_elements(By.CSS_SELECTOR, \"div.title\")\n",
    "\n",
    "#Container for all the urls\n",
    "url_bank = []\n",
    "\n",
    "for i, title_div in enumerate(TITLE_DIVS):\n",
    "    try:\n",
    "        # Locate the inner anchor element within the <div>\n",
    "        inner_anchor = title_div.find_element(By.CSS_SELECTOR, \"a\")\n",
    "\n",
    "        # Get the href attribute from the inner anchor element\n",
    "        href_value = inner_anchor.get_attribute(\"href\")\n",
    "\n",
    "        #Append URL to container\n",
    "        url_bank.append(href_value)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "beach_dict['urls'] = url_bank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need beach coordinates for future features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude_container = []\n",
    "longitude_container = []\n",
    "\n",
    "#Go through all the links, extract the latitude and longitude and save it to the beach_dict\n",
    "for name_link in beach_dict['urls']:\n",
    "    try:\n",
    "        beach_driver = anti_ad(name_link)\n",
    "        latitude_element = beach_driver.find_element(By.CSS_SELECTOR, 'meta[property=\"place:location:latitude\"]')\n",
    "        longitude_element = beach_driver.find_element(By.CSS_SELECTOR, 'meta[property=\"place:location:longitude\"]')\n",
    "\n",
    "        latitude_container.append(latitude_element.get_attribute(\"content\"))\n",
    "        longitude_container.append(longitude_element.get_attribute(\"content\"))\n",
    "        driver.quit()\n",
    "    except TimeoutException:\n",
    "        driver.quit()\n",
    "        raise RuntimeError(f\"{name_link} is broken or the required elements were not found\")\n",
    "\n",
    "beach_dict.update({'latitude': latitude_container, 'longitude': longitude_container})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "beach_info = pd.DataFrame(beach_dict)\n",
    "beach_info.to_csv('beach_info.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scuba_map",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
